"""Import the metadata sqlite3 database generated by the OF-Scraper project into a Stash app instance."""

import argparse
import asyncio
import json
import logging
import os
import re
import shutil
import signal
import sqlite3
import sys
import tempfile
import time
import typing as t
from ast import Raise
from concurrent.futures import ThreadPoolExecutor, as_completed

# from datetime import date, datetime, timedelta, timezone
from datetime import datetime
from functools import reduce
from glob import glob
from html import unescape
from importlib.metadata import version
from io import StringIO
from pprint import PrettyPrinter, pformat
from string import Formatter
from typing import Any
from urllib.parse import urlparse

import aiosqlite
import dateutil.parser  # type: ignore[unused-ignore]
import yaml  # type: ignore[unused-ignore]
from markdown import Markdown
from numpy.random import default_rng
from stashapi.stash_types import OnMultipleMatch

# import arrow
from stashapi.stashapp import StashInterface
from stashapi.tools import file_to_base64
from tqdm.asyncio import tqdm as progressbar

# from aiomultiprocess import Pool

logger = logging.getLogger(__name__)
FORMAT = (
    "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s-%(lineno)d - %(message)s"
)
LOG_FILE = f"debug-{datetime.now().strftime('%Y-%m-%d_%H-%M')}.log"
file_logger = logging.getLogger("file_logger")
logging.addLevelName(5, "TRACE")
file_handler = logging.FileHandler(LOG_FILE, mode="a", encoding="utf-8")
file_handler.setFormatter(logging.Formatter(FORMAT))
file_logger.addHandler(file_handler)
file_logger.setLevel(logging.DEBUG)
file_logger.propagate = False
runtime_settings = {}
semaphore = asyncio.Semaphore(16)
rng = default_rng()


class Default(dict[Any, Any]):
    def __missing__(self, key: str) -> str:
        return "{" + key + "}"


async def aiter_list(iterable: list) -> t.AsyncIterator[list]:
    for i in iterable:
        yield i


def signal_handler(sig, frame):
    print("Ctrl+C detected. Stopping the thread.")
    exit(130)


pprint = PrettyPrinter(
    indent=4, depth=2, compact=True, width=(os.get_terminal_size().columns - 5)
).pprint


def unmark_element(element, stream=None):
    if stream is None:
        stream = StringIO()
    if element.text:
        stream.write(element.text)
    for sub in element:
        unmark_element(sub, stream)
    if element.tail:
        stream.write(element.tail)
    return stream.getvalue()


Markdown.output_formats["plain"] = unmark_element
__md = Markdown(output_format="plain")
__md.stripTopLevelTags = False


def update_progress_bar(future: asyncio.Future, bar: progressbar) -> None:
    """
    Callback function to update a tqdm progress bar upon task completion.

    Args:
        future (Future): The future object that holds the result of an asynchronous operation.
        bar (tqdm): The tqdm progress bar instance to update.
    """
    result = future.result()  # Get the result of the future
    bar.update(result)  # Update the progress bar with the number of items processed


def convert_datetime(val: bytes) -> datetime:
    """Convert ISO 8601 datetime to datetime.datetime object."""
    return datetime.fromisoformat(val.decode())


# Stolen from https://stackoverflow.com/a/46890853 with some added type casting
def deep_get(
    dictionary: dict, keys: str, default: type[ValueError] | None = None
) -> Any:
    return reduce(
        lambda d, key: d.get(key, default) if isinstance(d, dict) else default,
        keys.split("."),
        dictionary,
    )


def load_config(config_file: str) -> None:
    global runtime_settings
    file_logger.debug(f"Loading config file: {config_file}")
    _, ext = os.path.splitext(config_file)
    with open(config_file) as file:
        if ext.lower() == ".json":
            data = json.load(file)
        elif ext.lower() == ".yaml" or ext.lower() == ".yml":
            data = yaml.safe_load(file)
        else:
            raise ValueError(
                f"Unsupported file extension {ext}. Please use .json or .yaml/.yml"
            )
    runtime_settings = data
    runtime_settings["config_file"] = config_file


async def sleep(seconds: int, printDelay: bool = True) -> None:
    now: int = round(time.time())
    if printDelay:
        logger.info(f"Sleeping for {seconds} seconds")
        logger.info(f"Resuming at {datetime.fromtimestamp(now + seconds)}")
    while time.time() < now + seconds:
        await asyncio.sleep(1)


def format_directory(dir_type: str | None = None, **vars) -> str:
    global runtime_settings
    return_string: str = ""
    if dir_type is None:
        raise ValueError("dir_type is required")
    if dir_type not in [
        "metadata_format",
        "save_location",
        "stash_location",
        "dir_format",
        "dir_format_stash",
    ]:
        raise ValueError(f"Unsupported dir_type: {dir_type}")
    path_key = (
        f"of_scraper.folders.{dir_type}"
        if dir_type != "dir_format_stash"
        else "of_scraper.folders.dir_format"
    )
    try:
        return_string = deep_get(runtime_settings, path_key, ValueError)
    except ValueError as e:
        file_logger.warning(f"Error: {e}")
        exit(1)
    if "metadata_format" in dir_type or "dir_format" in dir_type:
        save_or_stash = (
            "save_location"
            if "metadata_format" in dir_type or "dir_format" == dir_type
            else "stash_location"
        )
        return_string = return_string.format_map(
            Default(dict(**vars, save_location=format_directory(save_or_stash)))
        )
    fieldnames = [fname for _, fname, _, _ in Formatter().parse(return_string) if fname]
    replacements = {fname: vars.get("missing_values", "**") for fname in fieldnames}
    return_string = return_string.format_map(Default(dict(**replacements)))
    return return_string


def parse_media_row_to_studio_code(row: dict = {}) -> str:
    """
    Parse media row to get the link or filename
    and return the studio code.
    """
    file_logger.debug("%s", pformat(f"Row: {row}"))
    if not isinstance(row, dict):
        file_logger.error(f"row must be a dictionary:\n{pformat(row)}")
        raise ValueError("row must be a dictionary")
    if row == {}:
        file_logger.error(f"row cannot be empty: {row}")
        raise ValueError("row cannot be empty")
    if row["link"]:
        url = row["link"]
        converted_url = urlparse(url)
        converted_path = converted_url.path
        converted_array = converted_path.split("/")
        converted_filename = converted_array[-1]
        filename = converted_filename.split("?")[0]
        file_no_ext = os.path.splitext(filename)[0]
        if sys.version_info >= (3, 9):
            file_no_ext = file_no_ext.removesuffix("_source")
        else:
            file_no_ext = (
                file_no_ext[:-7] if file_no_ext.endswith("_source") else file_no_ext
            )
        file_logger.debug("%s", pformat(f"Filename: {file_no_ext}"))
        return file_no_ext
    else:
        pending_filename = os.path.splitext(row["filename"])[0]
        if "source" in pending_filename:
            full_filename = pending_filename.split("_")[-2].split("-")[-1]
        else:
            full_filename = pending_filename.split("_")[-1].split("-")[-1]
        if len(full_filename) < 5:
            full_filename = os.path.splitext(row["filename"])[0]
            file_logger.warning(
                pformat(f"Short path: {row['filename']} - trying full {full_filename}"),
            )
        return full_filename


async def load_db_into_memory(db_file: str) -> aiosqlite.Connection:
    # Create a temporary directory to store the local copy of the database
    file_logger.debug(f"Loading {db_file} into memory")
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            local_db_path = os.path.join(temp_dir, os.path.basename(db_file))
            # Copy the database file from the network drive to the local path
            shutil.copy(db_file, local_db_path)

            # Connect to the local copy of the database file
            async with aiosqlite.connect(
                local_db_path,
                detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
            ) as disk_conn:
                # Dump the database into SQL commands
                dump_commands = "".join(
                    [f"{line}\n" async for line in disk_conn.iterdump()]
                )
                # Connect to an in-memory database
                mem_conn = await aiosqlite.connect(
                    ":memory:",
                    detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
                )
                # Execute the dump commands to recreate the database in memory
                await mem_conn.executescript(dump_commands)
                return mem_conn  # Return the in-memory connection
    except sqlite3.Error as e:
        file_logger.warning(f"SQL Error: {db_file}")
        file_logger.exception(e, exc_info=False, stack_info=False)
        Raise(Exception(f"SQL Error: {db_file}"))


async def get_usernames_from_db(conn: aiosqlite.Connection, db_file: str) -> str:
    file_logger.debug(f"Getting username from {db_file}")
    try:
        async with conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='labels';"
        ) as cursor:
            if await cursor.fetchone() is None:
                username = db_file.split(os.sep)[-3]
                file_logger.info(
                    f"No profile table found in {db_file} so using {username}"
                )
                return username
            async with conn.execute("select username from profiles limit 1;") as cursor:
                row = await cursor.fetchone()
                if row:
                    return f"{row["username"]}"
                return db_file.split(os.sep)[-3]
    except sqlite3.Error as e:
        file_logger.warning(f"SQL Error: {db_file}")
        file_logger.exception(e, exc_info=False, stack_info=False)
        return db_file.split(os.sep)[-3]


async def get_metadata_db_files(
    models: list[str] | None = None,
    mod_date: str | None = None,
) -> list[tuple[str, aiosqlite.Connection]]:
    global runtime_settings
    file_logger.debug("%s", pformat(f"Models: {models}"))
    db_set: set[str] = set()
    if models:
        logger.info(f"Models specified: {models}")
        for model in models:
            file_logger.debug("%s", pformat(f"Model: {model}"))
            globList = glob(
                os.path.join(
                    format_directory(
                        dir_type="metadata_format",
                        model_username=model,
                        missing_values="**",
                    ),
                    "user_data.db",
                ),
                recursive=True,
            )
            file_logger.log(5, "%s", pformat(f"globList: {globList}"))
            if not (len(globList) > 0):
                file_logger.info(f"No metadata database files found for {model}")
                continue
            if mod_date:
                for db_file in globList:
                    if (
                        os.path.getmtime(db_file)
                        > dateutil.parser.parse(mod_date).timestamp()
                    ):
                        db_set.add(db_file)
            db_set.update(globList)
    else:
        logger.info("Finding all models in metadata directory")
        if runtime_settings["verbose"]:
            file_logger.info("No models specified")
        db_set_holder = set(
            glob(
                os.path.join(
                    format_directory(dir_type="metadata_format", missing_values="**"),
                    "user_data.db",
                ),
                recursive=True,
            )
        )
        if mod_date:
            for db_file in db_set_holder:
                if (
                    os.path.getmtime(db_file)
                    > dateutil.parser.parse(mod_date).timestamp()
                ):
                    db_set.add(db_file)
        else:
            db_set.update(db_set_holder)
    aiosqlite.register_converter("timestamp", convert_datetime)
    aiosqlite.register_converter("created_at", convert_datetime)
    return_list = []
    logger.info(f"Loading {len(db_set)} databases into memory")
    file_logger.debug("%s", pformat(f"DB Set: {db_set}"))
    for db_file in db_set:
        try:
            conn = await load_db_into_memory(db_file)
        except sqlite3.Error as e:
            file_logger.warning(f"SQL Error: {db_file}")
            file_logger.exception(e, exc_info=False, stack_info=False)
            continue
        except Exception as e:
            file_logger.error(f"Error: {e}")
            continue
        if conn is None:
            file_logger.error(f"Connection is None for {db_file}")
            logger.error(f"Connection is None for {db_file}")
            continue
        conn.row_factory = aiosqlite.Row
        username = await get_usernames_from_db(conn, db_file)
        return_list.append((username, conn))
    return return_list


async def create_performer(
    performer: str, client: StashInterface = None, **kwargs
) -> dict:
    variables = {
        "name": performer,
        "url": f"https://onlyfans.com/{performer}",
    }
    if kwargs["avatar"]:
        variables["avatar"] = file_to_base64(kwargs["avatar"])
    if kwargs["alias"]:
        variables["alias_list"] = kwargs["alias"]
    if kwargs["details"]:
        variables["details"] = kwargs["details"]
    result = client.create_performer(variables)
    return result


async def update_performer_avatar(
    performer: str = None, client: StashInterface = None
) -> dict:
    logger.info(f"Updating avatar for performer {performer}")
    performer_path = format_directory(dir_type="dir_format", model_username=performer)
    file_logger.debug("%s", pformat(f"Performer path: {performer_path}"))
    performer_path_list = os.path.normpath(performer_path).split(os.path.sep)[:-3]
    performer_path_list.extend(["Profile", "Free", "Images"])
    file_logger.debug("%s", pformat(f"Performer path list: {performer_path_list}"))
    performer_path_avatar = os.path.sep + os.path.join(*performer_path_list)
    avatars = []
    avatar_path = ""
    if os.path.exists(performer_path_avatar):
        avatars = glob(os.path.join(performer_path_avatar, "*avatar*"), recursive=True)
        file_logger.debug("%s", pformat(f"Avatars: {avatars}"))
    else:
        file_logger.debug(
            "%s",
            pformat(
                f"Avatar path {performer_path_avatar} doesn't exist, falling back to old path"
            ),
        )
        performer_path_list = os.path.normpath(performer_path).split(os.path.sep)[:-3]
        performer_path_list.extend(["Profile", "Avatars"])
        performer_path_avatar = os.path.sep + os.path.join(*performer_path_list)
        file_logger.debug("%s", pformat(f"Avatar path: {performer_path_avatar}"))
        avatars = glob(os.path.join(performer_path_avatar, "*.j*p*g"), recursive=True)
        file_logger.debug("%s", pformat(f"Avatars: {avatars}"))
    if len(avatars) > 1:
        logger.info(f"Multiple avatars found for performer {performer}.")
        for index, value in enumerate(avatars):
            print(f"{index} Avatar: {value}")
        avatar_select = ""
        while avatar_select not in [index for index in range(len(avatars))]:
            avatar_select = int(
                input("Please enter the number of the avatar picture to use: ")
            )
        avatar_path = avatars[avatar_select]
    elif len(avatars) == 1:
        avatar_path = avatars[0]
    if avatar_path != "":
        logger.info(f"Uploading avatar for performer {performer}: {avatar_path}")
        temp_performer = client.find_performer(
            performer=performer,
            fragment="id name disambiguation alias_list image_path url details",
        )
        file_logger.debug("Removing image_path from temp_performer")
        del temp_performer["image_path"]
        file_logger.debug("Adding image to temp_performer")
        temp_performer["image"] = file_to_base64(avatar_path)
        file_logger.debug("Updating performer")
        run_perf = client.update_performer(temp_performer)
        file_logger.debug("Checking if performer was updated")
        if run_perf:
            return run_perf
        else:
            return temp_performer
    return client.find_performer(
        performer=performer,
        fragment="id name disambiguation alias_list image_path url details",
    )


async def check_studio_description(
    studio: dict, performer: str, client: StashInterface = None
) -> dict:
    file_logger.debug(
        f"Checking description for performer {performer} - studio {studio}"
    )
    performer_path = format_directory(dir_type="dir_format", model_username=performer)
    file_logger.debug("%s", pformat(f"Performer path: {performer_path}"))
    performer_path_list = os.path.normpath(performer_path).split(os.path.sep)[:-3]
    performer_path_list.extend(["Profile", "Free", "Text"])
    profile_text_path = os.path.sep + os.path.join(*performer_path_list)
    file_logger.debug("%s", pformat(f"Performer text path: {profile_text_path}"))
    text_files = glob(os.path.join(profile_text_path, "*.txt"), recursive=True)
    file_logger.debug("%s", pformat(f"Text files: {text_files}"))
    if studio["details"] is None or studio["details"] == "":
        decide = ""
        pprint(f"Studio {studio['name']} has no description (details field in Stash).")
        if len(text_files) > 0:
            decide = ""
            while decide not in ["y", "n"]:
                decide = input(
                    f"Description found in {text_files[0]}. Do you want to add it? (y/n): "
                )
            if decide == "y":
                with open(text_files[0]) as file:
                    temp_studio = {"id": studio["id"]}
                    details = file.read()
                    file_logger.log(5, "%s", pformat(f"Details (direct): {details}"))
                    details = unescape(details).replace("<br /> ", "\n")
                    details = re.sub(r"<[^>]*>", "", details)
                    file_logger.log(5, "%s", pformat(f"Details (cleaned): {details}"))
                    temp_studio["details"] = details
                    return client.update_studio(temp_studio)
        if decide == "n" or decide == "":
            decide = ""
            while decide not in ["y", "n"]:
                decide = input("Do you want to add one? (y/n): ")
                if decide == "y":
                    temp_studio = {"id": studio["id"]}
                    details = input("Enter a description for the performer: ")
                    temp_studio["details"] = details
                    return client.update_studio(temp_studio)
    return studio


async def scan_performer_directory(performer: str, stash: StashInterface) -> None:
    global runtime_settings
    performer_path = format_directory(
        dir_type="dir_format_stash", model_username=performer, missing_values=""
    )
    performer_path = os.path.normpath(performer_path)
    file_logger.debug("%s", pformat(f"Performer path: {performer_path}"))
    scan_flags = {
        "scanGenerateCovers": True,
        "scanGeneratePreviews": True,
        "scanGenerateImagePreviews": True,
        "scanGenerateSprites": True,
        "scanGeneratePhashes": True,
        "scanGenerateThumbnails": True,
        "scanGenerateClipPreviews": True,
    }
    if runtime_settings["after"]:
        scan_flags["filter"] = {"minModTime": runtime_settings["after"]}
    file_logger.debug("%s", pformat(f"Scan flags: {scan_flags}"))
    prior_status: str = ""
    running_jobs: list[int] = []
    running_jobs = stash.job_queue() or []
    if len(running_jobs) > 0:
        logger.info(f"There are currently {len(running_jobs)} jobs in the queue.")
        file_logger.debug("%s", pformat(f"Running jobs: {running_jobs}"))
    job = stash.metadata_scan(
        paths=[performer_path],
        flags=scan_flags,
    )
    running_job = True
    while running_job:
        job_status = stash.find_job(job)
        if job_status["status"] in ["FINISHED", "FAILED", "CANCELLED"]:
            running_job = False
        else:
            if prior_status == job_status["status"] and job_status["status"] == "READY":
                prior_status = job_status["status"]
                time.sleep(0.5)
                continue
            try:
                logger.info(
                    f"Job status: {job_status['status']} - {job_status['description']} - Sub-Task: {job_status['subTasks']}"
                )
            except KeyError:
                logger.info(
                    f"Job status: {job_status['status']} - {job_status['description']}"
                )
            except TypeError:
                logger.info(
                    f"Job status: {job_status['status']} - {job_status['description']}"
                )
            time.sleep(0.5)
            prior_status = job_status["status"]


async def get_stash_performers(  # noqa: C901
    performers: list[str], client: StashInterface = None
) -> dict:
    gql_performers = {}
    file_logger.log(5, "%s", pformat(f"Performers: {performers}"))
    for performer in performers:
        result = client.find_performer(
            performer=performer,
            fragment="id name disambiguation alias_list image_path url",
        )
        file_logger.log(5, "%s", pformat(f"Performer {performer}: {result}"))
        gql_performers.update({performer: result})
    for performer in performers:
        if isinstance(gql_performers.get(performer, None), list):
            if len(gql_performers[performer]) > 1:
                logger.info(
                    "%s",
                    pformat(
                        f"Multiple performers found for {performer}.......{gql_performers[performer]}"
                    ),
                )
                count = sum(
                    1
                    for p in gql_performers[performer]
                    if p[0]["url"] == f"https://onlyfans.com/{performer}"
                )
                if count == 1:
                    for p in gql_performers[performer]:
                        if p["url"] == f"https://onlyfans.com/{performer}":
                            gql_performers[performer] = p
                continue
            else:
                gql_performers[performer] = gql_performers[performer][0]
        else:
            if gql_performers[performer] is None:
                decide = ""
                while decide not in ["y", "n"]:
                    decide = input(
                        f"Performer {performer} not found in Stash. Do you want to create a new performer? (y/n): "
                    )
                if decide == "y":
                    run_perf = await create_performer(
                        performer, client, avatar=None, alias=None, details=None
                    )
                    file_logger.log(5, "%s", pformat(f"run_perf: {run_perf}"))
                    if run_perf:
                        gql_performers[performer] = run_perf
                    continue
                else:
                    file_logger.log(5, f"Removing {performer} from gql_performers")
                    del gql_performers[performer]
                    continue
    for performer in performers:
        if isinstance(gql_performers.get(performer, None), dict):
            if "default=true" in gql_performers[performer]["image_path"]:
                decide = ""
                while decide not in ["y", "n"]:
                    decide = input(
                        f"Performer {performer} has no avatar. Do you want to upload one? (y/n): "
                    )
                if decide == "y":
                    gql_performers[performer] = await update_performer_avatar(
                        performer, client
                    )
    return gql_performers


async def create_studio(performer: str, client: StashInterface = None) -> dict:
    logger.info(f"Creating studio for performer {performer}")
    of_id = await get_of_studio_id(client)
    variables = {
        "name": f"{performer} (OnlyFans)",
        "url": f"https://onlyfans.com/{performer}",
        "parent_id": of_id,
    }
    file_logger.debug("%s", pformat(f"Variables: {variables}"))
    result = client.create_studio(variables)
    return result


async def get_of_studio_id(client: StashInterface = None) -> int:
    file_logger.log(5, "Getting OnlyFans studio ID")
    variables = {
        "filter": {"q": ""},
        "studio_filter": {
            "name": {"modifier": "EQUALS", "value": "OnlyFans (network)"},
        },
    }
    result = client.find_studios(
        f=variables["studio_filter"], filter=variables["filter"], fragment="id name url"
    )
    file_logger.log(5, "%s", pformat(f"Result: {result}"))
    return result[0]["id"]


async def get_stash_studio(performer: str, client: StashInterface = None) -> dict:
    global runtime_settings
    file_logger.debug(f"Getting studio for performer {performer}")
    variables = {
        "filter": {"q": "", "sort": "name", "direction": "ASC", "per_page": -1},
        "studio_filter": {
            "name": {"modifier": "EQUALS", "value": f"{performer} (OnlyFans)"},
        },
    }
    result = client.find_studios(
        f=variables["studio_filter"], filter=variables["filter"], get_count=True
    )
    file_logger.debug("%s", pformat(f"Result: {result}"))
    if result[0] > 1:
        decide = ""
        while decide not in [studio["id"] for studio in result[1]]:
            print(f"Multiple studios found for performer {performer}.")
            for studio in result[1]:
                print(f"ID: {studio['id']} - Name: {studio['name']}")
            decide = input("Please enter the ID of the correct studio:")
            for studio in result[1]:
                if studio["id"] == decide:
                    return studio
    elif result[0] == 0:
        decide = ""
        while decide not in ["y", "n"]:
            decide = input(
                f"No studios found for performer {performer}. Do you want to create one? (y/n): "
            )
        if decide == "y":
            return await create_studio(performer, client)
        return None
    return result[1][0]


async def verify_studio_url(
    studio: dict, performer: str, client: StashInterface = None
) -> dict:
    file_logger.debug(f"Verifying URL for studio {studio}")
    if studio is None:
        return None
    if studio["url"] != f"https://onlyfans.com/{performer}":
        print(f"Studio {studio['name']} has an incorrect URL")
        print(f"Current URL: {studio['url']}")
        print(f"Correct URL: https://onlyfans.com/{performer}")
        decide = ""
        while decide not in ["y", "n"]:
            decide = input(f"Do you want to update the URL for {performer}? (y/n): ")
        if decide == "y":
            variables = {
                "id": studio["id"],
                "name": studio["name"],
                "url": f"https://onlyfans.com/{performer}",
            }
            return client.update_studio(variables)
    return studio


# Function to create nested OR conditions
def create_nested_path_or_conditions(
    conditions: list[dict[str, any]]
) -> dict[str, dict]:
    """
    Recursively nests conditions using 'OR' without creating unnecessary lists.
    """
    if len(conditions) == 1:
        return conditions[0]
    if len(conditions) == 2:
        return {
            "OR": {
                "path": conditions[0]["path"],
                "OR": conditions[1],
            }
        }
    else:
        # Use recursion to nest the conditions directly into each other without forming a list
        return {
            "OR": {
                "path": conditions[0]["path"],
                "OR": create_nested_path_or_conditions(conditions[1:])["OR"],
            }
        }


def process_db_row_image_files(
    rows: list[dict[str, any]], performer: str = None, sleep: bool = True
) -> list[dict[str, any]]:
    global runtime_settings
    if sleep:
        time_to_sleep = rng.uniform(0, 3)
        time.sleep(time_to_sleep)
    stash = StashInterface(
        {
            "scheme": runtime_settings["stashapp"]["scheme"],
            "host": runtime_settings["stashapp"]["host"],
            "port": runtime_settings["stashapp"]["port"],
            "ApiKey": runtime_settings["stashapp"]["api_key"],
        }
    )
    file_logger.log(5, "%s", pformat(f"Rows: {rows}"))
    if len(rows) == 0:
        return []
    elif isinstance(rows, dict):
        rows = [rows]
    paths = []
    for row in rows:
        if row["filename"]:
            path = ""
            if not any(
                substring in row["filename"] for substring in ["header", "avatar"]
            ):
                path = (os.path.splitext(row["filename"])[0]).split("_")[-1]
                if len(path) < 5:
                    full_filename = os.path.splitext(row["filename"])[0]
                    file_logger.debug(
                        "%s",
                        pformat(f"Short path: {path} - trying full {full_filename}"),
                    )
                    path = full_filename
                paths.append(path)
    if len(paths) == 0:
        return []
    or_conditions = [
        {"path": {"modifier": "INCLUDES", "value": path}} for path in paths if path
    ]
    nested_or_conditions = create_nested_path_or_conditions(or_conditions)
    filter_params = {"per_page": -1, "q": performer}
    fragment = """id
        title
        code
        details
        urls
        date
        organized
        created_at
        updated_at
        files {
            id
            path
            basename
            parent_folder_id
            size
            width
            height
            created_at
            updated_at
            }
        galleries {
            id
            }
        studio {
            id
            }
        tags {
            id
            }
        performers {
            id
            }
        """
    images = stash.find_images(
        f=nested_or_conditions, filter=filter_params, fragment=fragment
    )
    for row in rows:
        if row["text"]:
            row["text"] = unescape(row["text"]).replace("<br /> ", "\n")
            row["text"] = re.sub(r"<[^>]*>", "", row["text"])
        if row["filename"] is None:
            continue
        file_id = os.path.splitext(row["filename"])[0]
        row["files"] = []
        for image in images:
            for file in image["files"]:
                if file_id in file["path"]:
                    row["files"].append(image)
    file_logger.log(
        10, "%s", pformat(f"Files processed: {[row['filename'] for row in rows]}")
    )
    return rows


def searchPerformers(scene):
    pattern = re.compile(r"(?:^|\s)@([\w\-\.]+)")
    content = unescape(scene["details"])
    # if title is truncated, remove trailing dots and skip searching title
    if scene["title"].endswith("..") and scene["title"].removesuffix("..") in content:
        searchtext = content
    else:
        # if title is unique, search title and content
        searchtext = scene["title"] + " " + content
    usernames = re.findall(pattern, unescape(searchtext))
    return usernames


def sanitize_string(string):
    """
    Parses and sanitizes strings to remove HTML tags
    """
    if string:
        string = unescape(string).replace("<br /> ", "\n")
        string = re.sub(r"<[^>]*>", "", string)
        return string
    return string


def truncate_title(title, max_length):
    """
    Truncate title to provided maximum length while preserving word boundaries.
    """
    # Check if the title is already within the desired length
    if len(title) <= max_length:
        return title

    # Find the last space character before the max length
    last_space_index = title.rfind(" ", 0, max_length)
    # If there's no space before the max length, simply truncate the string
    if last_space_index == -1:
        return title[:max_length]
    # Otherwise, truncate at the last space character
    return title[:last_space_index]


def remove_markdown_in_title(title):
    """
    Remove markdown characters in the title.
    """
    return __md.convert(title)


def format_title(title, username, date, scene_index, scene_count):
    """
    Format a post title based on various conditions.
    """
    if len(title) == 0:
        scene_info = f" ({scene_index})" if scene_index > 0 else ""
        return f"{username} - {date}{scene_info}"

    title = sanitize_string(title)

    title = remove_markdown_in_title(title)

    f_title = truncate_title(
        title.split("\n")[0].strip(), runtime_settings["max_title_length"]
    )
    scene_info = f" ({scene_index})" if scene_index > 0 else ""

    if len(f_title) <= 5:
        return f"{f_title} - {date}{scene_info}"

    if not bool(re.search("[A-Za-z0-9]", f_title)):
        if scene_index == 0:
            title_max_len = runtime_settings["max_title_length"] - 13
        else:
            title_max_len = (
                runtime_settings["max_title_length"] - 16 - len(str(scene_index))
            )
        t_title = truncate_title(f_title, title_max_len)
        scene_info = f" ({scene_index})" if scene_index > 0 else ""
        return f"{t_title} - {date}{scene_info}"

    scene_info = f" {scene_index}/{scene_count}" if scene_index > 0 else ""
    return f"{f_title}{scene_info}"


def process_db_row_scene_files(
    rows: list[dict[str, any]], performer: str = None, sleep: bool = True
) -> dict:
    global runtime_settings
    if sleep:
        time_to_sleep = rng.uniform(0, 3)
        time.sleep(time_to_sleep)
    stash = StashInterface(
        {
            "scheme": runtime_settings["stashapp"]["scheme"],
            "host": runtime_settings["stashapp"]["host"],
            "port": runtime_settings["stashapp"]["port"],
            "ApiKey": runtime_settings["stashapp"]["api_key"],
        }
    )
    file_logger.log(5, "%s", pformat(f"Count of rows in this loop: {len(rows)}"))
    file_logger.log(5, "%s", pformat(f"Rows: {rows}"))
    if len(rows) == 0:
        return []
    elif isinstance(rows, dict):
        rows = [rows]
    paths = []
    for row in rows:
        path = ""
        try:
            path = parse_media_row_to_studio_code(row)
            row["studio_code"] = path
        except ValueError as e:
            file_logger.warning(f"Error: {e}")
            continue
        except Exception as e:
            file_logger.error(f"Error: {e}")
            continue
        paths.append(path)
    or_conditions = [
        {"path": {"modifier": "INCLUDES", "value": path}} for path in paths
    ]
    file_logger.log(5, "%s", pformat(f"or_conditions: {or_conditions}"))
    nested_or_conditions = create_nested_path_or_conditions(or_conditions)
    filter_params = {"per_page": -1, "q": performer}
    fragment = """id
        title
        code
        details
        director
        urls
        date
        organized
        created_at
        updated_at
        files {
            id
            path
            basename
            parent_folder_id
            size
            format
            width
            height
            duration
            created_at
            updated_at
            }
        galleries {
            id
            }
        studio {
            id
            }
        tags {
            id
            }
        performers {
            id
            }
        """
    stash_scenes = stash.find_scenes(
        f=nested_or_conditions, filter=filter_params, get_count=True, fragment=fragment
    )
    file_logger.log(5, "%s", pformat(f"Scenes: {stash_scenes}"))
    if stash_scenes[0] == 0:
        file_logger.info("%s", pformat(f"No scenes found for {paths}"))
        return []
    if stash_scenes[0] > len(paths):
        rows = []
        file_logger.error(f"More scenes found than paths:\n{pformat(stash_scenes[1])}")
        Raise(ValueError(f"More scenes found than paths: {pformat(stash_scenes[1])}"))
    for row in rows:
        file_id = None
        if row["text"]:
            row["text"] = unescape(row["text"]).replace("<br /> ", "\n")
            row["text"] = re.sub(r"<[^>]*>", "", row["text"])
        file_id = row["studio_code"]
        if file_id is None:
            continue
        row["scene"] = []
        for scene in stash_scenes[1]:
            for file in scene["files"]:
                if file_id in file["path"] and scene["organized"] is False:
                    row["scene"].append(scene)
    file_logger.log(5, "%s", pformat(f"Rows to be returned: {rows}"))
    file_logger.log(
        10, "%s", pformat(f"Files processed: {[row['filename'] for row in rows]}")
    )
    return rows


# Refactor the process_image_files function to be less complex
async def get_medias_from_db(
    conn: aiosqlite.Connection, **kwargs
) -> list[dict[str, any]]:
    api_type = kwargs.get("api_type", "Posts")
    media_type = kwargs.get("media_type", "Images")
    page_size = kwargs.get("page_size", 25)
    username = kwargs.get("username", None)
    file_logger.info(
        f"Getting {media_type} - {api_type} for {username} with page size {page_size}"
    )
    base_sql_template = """SELECT {select_columns} from main.medias {join_options} where medias.media_type = '{media_type}' and medias.downloaded == '1' and medias.api_type {api_types} {date_filters}"""
    api_types_mapping = {
        "Posts": ("posts", "IN ('Timeline','Posts','Pinned','Archived')"),
        "Messages": ("messages", "IN ('Messages','Message','Paid')"),
        "Stories": ("stories", "IN ('Stories','Highlights')"),
        "Others": (
            "others",
            "NOT IN ('Stories','Highlights','Messages','Message','Timeline','Posts','Pinned','Archived')",
        ),
    }
    table, api_types = api_types_mapping.get(api_type, (None, None))
    if table is None or api_types is None:
        raise ValueError(f"Unsupported api_type: {api_type}")
    process_functions = {
        "Images": process_db_row_image_files,
        "Videos": process_db_row_scene_files,
    }

    process_function = process_functions.get(media_type)
    if not process_function:
        raise ValueError(f"Unsupported media_type: {media_type}")

    before = kwargs.get("before", None)
    after = kwargs.get("after", None)
    date_template = " AND {table}.created_at {date}"
    if before or after:
        date_filters = ""
        if before:
            date_filters += date_template.format(table=table, date=f"< '{before}'")
        if after:
            date_filters += date_template.format(table=table, date=f"> '{after}'")
    else:
        date_filters = ""

    medias = []
    # Fetch total count first
    base_sql_command = base_sql_template.format(
        select_columns="COUNT(*) OVER()",
        join_options="LEFT JOIN {table} ON medias.post_id = {table}.post_id",
        media_type=media_type,
        api_types=api_types,
        date_filters=date_filters,
    ).format(table=table)
    file_logger.log(5, "%s", pformat(f"SQL Command: {base_sql_command}"))
    async with conn.execute(f"{base_sql_command} LIMIT 1") as cursor:
        total_row = await cursor.fetchone()
        total_count = total_row[0] if total_row else 0

    if total_count == 0:
        file_logger.info(
            "%s",
            pformat(
                f"No {media_type} - {api_type} found for {username} during the specified period: {after} - {before}"
            ),
        )
        return medias
    file_logger.debug(
        f"Processing {total_count} {media_type} - {api_type} for {username}"
    )
    total_pages = (total_count + page_size - 1) // page_size
    file_logger.debug(
        f"Processing {total_pages} pages of {page_size} {media_type} - {api_type} for {username}"
    )
    bar = progressbar(total=total_count, desc=f"{username} {api_type}", position=0)
    task_addition_bar = progressbar(
        total=total_pages, desc=f"{username} {api_type} - Adding Tasks", position=1
    )
    base_sql_command = base_sql_template.format(
        select_columns="medias.filename, medias.downloaded, medias.post_id, medias.media_id, medias.api_type, {table}.text, {table}.created_at, medias.link, medias.linked",
        join_options="LEFT JOIN {table} ON medias.post_id = {table}.post_id",
        media_type=media_type,
        api_types=api_types,
        date_filters=date_filters,
    ).format(table=table)
    file_logger.log(5, "%s", pformat(f"SQL Command: {base_sql_command}"))
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for page in range(total_pages):
            offset = page * page_size
            paginated_sql_command = f"{base_sql_command} ORDER BY medias.post_id DESC LIMIT {page_size} OFFSET {offset}"
            async with conn.execute(paginated_sql_command) as cursor:
                rows = await cursor.fetchall()
                if not rows:
                    continue
                rows = [dict(row) for row in rows]
                sleep = True
                if page < 15:
                    sleep = False
                future = executor.submit(
                    process_function, rows=rows, performer=username, sleep=sleep
                )
                futures.append(future)
            task_addition_bar.update(1)
        task_addition_bar.close()
        for future in as_completed(futures):
            result = future.result()  # Get the result of the completed task
            if api_type == "Messages":
                for row in result:
                    row["api_type"] = "Messages"
            bar.update(
                len(result)
            )  # Update the progress bar with the number of items processed
            medias.extend(result)
        bar.close()
    return medias


async def group_medias_by_post_id(
    medias: list[dict[str, any]]
) -> dict[any, list[dict[str, any]]]:
    # Code to group medias by post_id goes here
    grouped_medias: dict[tuple[any, any]] = {}
    async for media in aiter_list(medias):
        try:
            post_id = media["post_id"]
            if post_id not in grouped_medias:
                grouped_medias[post_id] = []
            grouped_medias[post_id].append(media)
        except TypeError:
            pprint(media)
            continue
    return grouped_medias


async def get_post_labels_from_db(
    conn: aiosqlite.Connection, post_id: any
) -> list[tuple[int, str]]:
    labels = [(None, None)]
    async with conn.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='labels';"
    ) as cursor:
        rows = await cursor.fetchall()
        if len(rows) == 0:
            return labels
    try:
        async with conn.execute(
            "select id,label_id,name from labels where post_id = ?;", (post_id,)
        ) as cursor:
            async for row in cursor:
                labels.append((row["label_id"], row["name"]))
    except sqlite3.Error as e:
        logger.exception(e, exc_info=False, stack_info=False)
    return labels


async def process_labels_to_tags(
    labels: list[str], stash: StashInterface
) -> dict[str, list[int]]:
    return_tags = {}
    file_logger.log(5, "%s", pformat(f"Labels: {labels}"))
    for label in labels:
        tags = []
        tag = stash.find_tag(
            tag_in=label,
            fragment="id",
            on_multiple=OnMultipleMatch.RETURN_LIST,
            create=True,
        )
        if isinstance(tag, list):
            if len(tag) > 1:
                pprint(f"Multiple tags found for {label} - {tag}")
            for i_tag in tag:
                tags.append(i_tag["id"])
            return_tags[label] = tags
        else:
            return_tags[label] = [tag["id"]]
    return return_tags


async def gather_model_labels(
    conn: aiosqlite.Connection, stash: StashInterface
) -> dict[int, list[tuple[int, str]]]:
    labels = {}
    conn.row_factory = aiosqlite.Row
    async with conn.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='labels';"
    ) as cursor:
        rows = await cursor.fetchall()
        if len(rows) == 0:
            file_logger.info("No labels table found in database")
            return labels
    try:
        async with conn.execute("select id from labels limit 1;") as cursor:
            rows = await cursor.fetchall()
            if len(rows) == 0:
                file_logger.info("No labels found in database")
                return labels
        async with conn.execute(
            "select DISTINCT name, label_id from labels;"
        ) as cursor:
            rows = await cursor.fetchall()
            rows = [dict(row) for row in rows]
            file_logger.log(5, "%s", pformat(f"Labels Rows: {rows}"))
            all_tags = await process_labels_to_tags(
                labels=[row["name"] for row in rows], stash=stash
            )
            file_logger.log(5, "%s", pformat(f"Tags: {all_tags}"))
        async with conn.execute(
            "SELECT post_id, name, label_id FROM labels order by post_id;"
        ) as cursor:
            rows = await cursor.fetchall()
            rows = [dict(row) for row in rows]
            for row in rows:
                if row["post_id"] not in labels:
                    labels[row["post_id"]] = []
                labels[row["post_id"]].extend(all_tags[row["name"]])
    except sqlite3.Error as e:
        logger.exception(e, exc_info=False, stack_info=False)
    return labels


async def create_or_update_image_galleries(
    grouped_medias: dict[any, list[dict[str, any]]], **kwargs
) -> None:
    username: str = kwargs.get("username", None)
    stash: StashInterface = kwargs.get("stash", None)
    performer: dict = kwargs.get("performer", None)
    gathered_tags: dict = kwargs.get("labels", None)
    if grouped_medias is None or len(grouped_medias) == 0:
        return
    bar = progressbar(
        total=len(grouped_medias), desc=f"Updating galleries for {username}"
    )
    performer_studio = performer.get("studio", False) or await get_stash_studio(
        username, stash
    )
    file_logger.log(5, "%s", pformat(f"Grouped Medias: {grouped_medias}"))
    for post_id, media_list in grouped_medias.items():
        tags: list = gathered_tags.get(post_id, [])
        if "api_type" in media_list[0]:
            if media_list[0]["api_type"] == "Messages":
                message_tag: dict = stash.find_tag(
                    tag_in="[FS: Messages]", fragment="id"
                )
                if message_tag is None:
                    message_tag = stash.create_tag({"name": "[FS: Messages]"})
                tags.append(message_tag["id"])
        current_gallery = stash.find_galleries(
            f={
                "code": {"modifier": "EQUALS", "value": f"{post_id}"},
                "performers": {"modifier": "INCLUDES", "value": [performer["id"]]},
            },
            fragment="""id title details date performers{id} studio{id} code urls tags{id}""",
        )
        if current_gallery is None or len(current_gallery) == 0:
            current_gallery = {}
            current_gallery["title"] = f"{username} - {post_id}"
            current_gallery["details"] = media_list[0]["text"]
            current_gallery["performer_ids"] = [performer["id"]]
            current_gallery["code"] = f"{post_id}"
            if media_list[0]["created_at"] is not None:
                current_gallery["date"] = media_list[0]["created_at"].strftime(
                    "%Y-%m-%d"
                )
            current_gallery["urls"] = [f"https://onlyfans.com/{post_id}/{username}"]
            current_gallery["studio_id"] = performer_studio["id"]
            current_gallery["tag_ids"] = tags
            file_logger.log(10, "%s", pformat(f"Create Gallery: {current_gallery}"))
            gallery_id = stash.create_gallery(current_gallery)
        else:
            file_logger.log(5, "%s", pformat(f"Current Gallery: {current_gallery}"))
            gallery_id = current_gallery[0]["id"]
            differences = {"id": gallery_id}
            if current_gallery[0]["title"] != f"{username} - {post_id}":
                differences["title"] = f"{username} - {post_id}"
            if (
                current_gallery[0]["details"] != media_list[0]["text"]
                and media_list[0]["text"] is not None
            ):
                differences["details"] = media_list[0]["text"]
            if media_list[0]["created_at"] is not None:
                if isinstance(media_list[0]["created_at"], str):
                    if current_gallery[0]["date"] != dateutil.parser.parse(
                        media_list[0]["created_at"]
                    ).strftime("%Y-%m-%d"):
                        differences["date"] = media_list[0]["created_at"]
                elif isinstance(media_list[0]["created_at"], datetime):
                    if current_gallery[0]["date"] != media_list[0][
                        "created_at"
                    ].strftime("%Y-%m-%d"):
                        differences["date"] = media_list[0]["created_at"].strftime(
                            "%Y-%m-%d"
                        )
                else:
                    Raise(ValueError(f"created_at date wrong type: {media_list[0]}"))
            else:
                Raise(ValueError(f"No created_at date: {media_list[0]}"))
            if {"id": performer["id"]} not in current_gallery[0]["performers"]:
                differences["performer_ids"] = [performer["id"]]
            if current_gallery[0]["studio"]["id"] != performer_studio["id"]:
                differences["studio_id"] = performer_studio["id"]
            if current_gallery[0]["code"] != f"{post_id}":
                differences["code"] = f"{post_id}"
            if current_gallery[0]["urls"] != [
                f"https://onlyfans.com/{post_id}/{username}"
            ]:
                differences["urls"] = [f"https://onlyfans.com/{post_id}/{username}"]
            tag_ids = [tag["id"] for tag in current_gallery[0]["tags"]]
            logger.debug("%s", pformat(f"Tag IDs: {tag_ids}"))
            if tag_ids != tags:
                differences["tag_ids"] = tags
            if len(differences) > 1:
                file_logger.debug("\n%s", pformat(f"Differences: {differences}"))
                updated = False
                while not updated:
                    try:
                        stash.update_gallery(differences)
                        updated = True
                    except Exception as e:
                        logger.exception(e, exc_info=False, stack_info=False)
                        logger.info(f"{differences} \n\n {current_gallery}")
                        await asyncio.sleep(rng.uniform(0, 3))
        file_ids = []
        stash_gallery_images_holder = stash.find_gallery_images(
            gallery_id=gallery_id, fragment="id"
        )
        if stash_gallery_images_holder is None:
            stash_gallery_images_holder = []
        gallery_images = [
            i["id"] for i in stash_gallery_images_holder if i["id"] is not None
        ]
        file_logger.log(
            5, "%s", pformat(f"Gallery Images - {gallery_id}: {gallery_images}")
        )
        for media in media_list:
            for file in media["files"] if "files" in media else []:
                try:
                    image_diff = {
                        "id": file["id"],
                    }
                    if "title" in file:
                        if file["title"] != f"{username} - {post_id}":
                            image_diff["title"] = f"{username} - {post_id}"
                            file_logger.debug(
                                "%s",
                                pformat(
                                    f"Title: {file['title']} - {image_diff['title']}"
                                ),
                            )
                    else:
                        image_diff["title"] = f"{username} - {post_id}"
                    if "details" in file:
                        if (
                            file["details"] != media_list[0]["text"]
                            and media_list[0]["text"] is not None
                        ):
                            image_diff["details"] = media_list[0]["text"]
                    else:
                        image_diff["details"] = ""
                    if media_list[0]["created_at"] is not None:
                        if file["date"] != media_list[0]["created_at"].strftime(
                            "%Y-%m-%d"
                        ):
                            image_diff["date"] = media_list[0]["created_at"].strftime(
                                "%Y-%m-%d"
                            )
                    else:
                        Raise(ValueError(f"No created_at date: {media_list[0]}"))
                    if performer["id"] not in [p["id"] for p in file["performers"]]:
                        image_diff["performer_ids"] = [
                            p["id"] for p in file["performers"]
                        ]
                        image_diff["performer_ids"].append(performer["id"])
                    if file["studio"] is not None:
                        if file["studio"]["id"] != performer_studio["id"]:
                            image_diff["studio_id"] = performer_studio["id"]
                    else:
                        image_diff["studio_id"] = performer_studio["id"]
                    if file["code"] != f"{post_id}":
                        image_diff["code"] = f"{post_id}"
                    if file["urls"] != [f"https://onlyfans.com/{post_id}/{username}"]:
                        image_diff["urls"] = [
                            f"https://onlyfans.com/{post_id}/{username}"
                        ]
                    tag_ids = {tag["id"] for tag in file["tags"]}
                    if tag_ids != set(tags):
                        image_diff["tag_ids"] = tags
                    if len(image_diff) > 1:
                        file_logger.debug("%s", pformat(f"File Diff: {image_diff}"))
                        updated = False
                        while not updated:
                            try:
                                stash.update_image(image_diff)
                                updated = True
                            except Exception as e:
                                logger.exception(e, exc_info=True, stack_info=False)
                                logger.info(f"{image_diff} \n\n {file} \n\n {media}")
                                await asyncio.sleep(rng.uniform(0, 3))
                    if file["id"] not in gallery_images:
                        file_ids.append(file["id"])
                except Exception as e:
                    logger.exception(e, exc_info=True, stack_info=True)
        if len(file_ids) > 0:
            file_logger.debug(
                "%s",
                pformat(f"For Gallery ID: {gallery_id} - Add File IDs: {file_ids}"),
            )
            stash.add_gallery_images(
                gallery_id=gallery_id,
                image_ids=file_ids,
            )
        bar.update(1)
    bar.close()


async def update_scene(
    grouped_medias: dict[any, list[dict[str, any]]], **kwargs
) -> None:
    username: str = kwargs.get("username", None)
    stash: StashInterface = kwargs.get("stash", None)
    performer: dict = kwargs.get("performer", None)
    gathered_tags: dict = kwargs.get("labels", None)
    if grouped_medias is None or len(grouped_medias) == 0:
        return
    bar = progressbar(total=len(grouped_medias), desc=f"Updating scenes for {username}")
    performer_studio = performer.get("studio", False) or await get_stash_studio(
        username, stash
    )
    for post_id, media_list in grouped_medias.items():
        tags: list = gathered_tags.get(post_id, [])
        if len(media_list) == 0:
            file_logger.debug("%s", pformat(f"No media found for {post_id}"))
            continue
        for media in media_list:
            if "api_type" in media:
                if media["api_type"] == "Messages":
                    message_tag: dict = stash.find_tag(
                        tag_in="[FS: Messages]", fragment="id"
                    )
                    if message_tag is None:
                        message_tag = stash.create_tag({"name": "[FS: Messages]"})
                    tags.append(message_tag["id"])

            file_logger.log(5, "%s", pformat(f"media: {media}"))
            files = media.get("scene", None)
            file_logger.log(5, "%s", pformat(f"Scenes: {files}"))
            if files is [] or files is None:
                file_logger.info("%s", pformat(f"No scenes found for {post_id}"))
                continue
            for file in files:
                if len(file) == 0:
                    file_logger.debug("%s", pformat(f"File is empty: {file}"))
                    continue
                if isinstance(file, dict):
                    file = [file]
                for scene in file:
                    differences = {"id": scene["id"]}
                    # scene_id = scene["id"]
                    if media["text"]:
                        title_holder = format_title(
                            scene["title"],
                            username,
                            media["created_at"].strftime("%Y-%m-%d"),
                            media_list.index(media),
                            len(media_list),
                        )
                        if scene["title"] != title_holder:
                            differences["title"] = title_holder
                    if media_list[0]["text"]:
                        if scene["details"] != media_list[0]["text"]:
                            differences["details"] = media_list[0]["text"]
                    if media_list[0]["created_at"] is not None:
                        if scene["date"] != media_list[0]["created_at"].strftime(
                            "%Y-%m-%d"
                        ):
                            differences["date"] = media_list[0]["created_at"].strftime(
                                "%Y-%m-%d"
                            )
                    else:
                        Raise(ValueError(f"No created_at date: {media_list[0]}"))
                    if performer["id"] not in [p["id"] for p in scene["performers"]]:
                        differences["performer_ids"] = [
                            p["id"] for p in scene["performers"]
                        ]
                        differences["performer_ids"].append(performer["id"])
                    if scene["studio"] is not None:
                        if scene["studio"]["id"] != performer_studio["id"]:
                            differences["studio_id"] = performer_studio["id"]
                    else:
                        differences["studio_id"] = performer_studio["id"]
                    if scene["code"] != media["studio_code"]:
                        differences["code"] = media["studio_code"]
                    if scene["urls"] != [f"https://onlyfans.com/{post_id}/{username}"]:
                        differences["urls"] = [
                            f"https://onlyfans.com/{post_id}/{username}"
                        ]
                    tag_ids = {tag["id"] for tag in scene["tags"]}
                    if tag_ids != set(tags):
                        differences["tag_ids"] = tags
                    if len(differences) > 1:
                        file_logger.log(10, "%s", pformat(f"scene (before): {scene}"))
                        file_logger.debug("%s", pformat(f"Differences: {differences}"))
                        diff_loop = False
                        while not diff_loop:
                            try:
                                stash.update_scene(differences)
                                diff_loop = True
                            except Exception as e:
                                logger.exception(e, exc_info=True, stack_info=False)
                                logger.info(f"{differences} \n\n {scene} \n\n {media}")
                                await asyncio.sleep(rng.uniform(0, 3))
        bar.update(1)
    bar.close()


async def process_image_files(db_file: aiosqlite.Connection, **kwargs) -> None:
    global runtime_settings
    stash: StashInterface = kwargs["stash"] or Raise(ValueError("stash is required"))
    performer: dict = kwargs["performer"] or Raise(ValueError("performer is required"))
    username: str = kwargs["username"] or Raise(ValueError("username is required"))

    page_size = 25
    # file_logger.debug("%s", pformat(f"performer: {performer}"))
    aiosqlite.register_converter("timestamp", convert_datetime)
    aiosqlite.register_converter("created_at", convert_datetime)
    medias = []
    db_file.row_factory = aiosqlite.Row
    try:
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Posts",
                media_type="Images",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Messages",
                media_type="Images",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Stories",
                media_type="Images",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Others",
                media_type="Images",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
    except sqlite3.Error as e:
        logger.exception(e, exc_info=False, stack_info=False)
        logger.info(f"{db_file} - Error: {e}")
        return None
    grouped_medias = await group_medias_by_post_id(medias)
    gathered_tags: dict = await gather_model_labels(db_file, stash)
    file_logger.log(5, "%s", pformat(f"Gathered Tags: {gathered_tags}"))
    file_logger.log(5, "%s", pformat(f"Grouped Medias: {grouped_medias}"))

    await create_or_update_image_galleries(
        grouped_medias,
        username=username,
        stash=stash,
        performer=performer,
        labels=gathered_tags,
    )


async def process_scene_files(db_file: aiosqlite.Connection, **kwargs) -> None:
    global runtime_settings
    stash: StashInterface = kwargs["stash"] or Raise(ValueError("stash is required"))
    performer: dict = kwargs["performer"] or Raise(ValueError("performer is required"))
    username: str = kwargs["username"] or Raise(ValueError("username is required"))
    page_size = 25
    file_logger.log(5, "%s", pformat(f"performer: {performer}"))
    aiosqlite.register_converter("timestamp", convert_datetime)
    aiosqlite.register_converter("created_at", convert_datetime)
    medias = []
    db_file.row_factory = aiosqlite.Row
    try:
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Posts",
                media_type="Videos",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Messages",
                media_type="Videos",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Stories",
                media_type="Videos",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
        medias.extend(
            await get_medias_from_db(
                db_file,
                username=username,
                api_type="Others",
                media_type="Videos",
                page_size=page_size,
                before=runtime_settings.get("before", None),
                after=runtime_settings.get("after", None),
            )
        )
    except sqlite3.Error as e:
        logger.exception(e, exc_info=False, stack_info=False)
        logger.info(f"{db_file} - Error: {e}")
        return None
    grouped_medias = await group_medias_by_post_id(medias)
    gathered_tags: dict = await gather_model_labels(db_file, stash)
    file_logger.log(5, "%s", pformat(f"Gathered Tags: {gathered_tags}"))
    file_logger.log(5, "%s", pformat(f"Grouped Medias: {grouped_medias}"))

    await update_scene(
        grouped_medias,
        username=username,
        stash=stash,
        performer=performer,
        labels=gathered_tags,
    )


def valid_date_format(date_str: str) -> str:
    try:
        datetime.strptime(date_str, "%Y-%m-%d")
        return date_str
    except ValueError:
        raise argparse.ArgumentTypeError(
            f"Invalid date format: {date_str}. Expected format YYYY-MM-DD."
        )


def parse_arguments() -> argparse.Namespace:
    """
    Parses command line arguments using argparse.

    Returns:
    argparse.NamespaceParsed arguments
    """
    parser = argparse.ArgumentParser(description="pyOFscraperStash App")
    parser.add_argument(
        "-a", "--after", type=valid_date_format, dest="after", help="After date"
    )
    parser.add_argument(
        "-b", "--before", type=valid_date_format, dest="before", help="Before date"
    )
    parser.add_argument(
        "-md",
        "--metadata-modification-date",
        type=valid_date_format,
        dest="metadata_modification_date",
        help="Get models with metadata modification date or after",
    )
    parser.add_argument(
        "-c", "--config-file", dest="config_file", help="Configuration file path"
    )
    parser.add_argument(
        "-i",
        "--images-only",
        dest="images_only",
        action="store_true",
        help="Process image files only",
    )
    parser.add_argument(
        "-s",
        "--scenes-only",
        dest="scenes_only",
        action="store_true",
        help="Process scene files only",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        dest="verbose",
        action="store_true",
        help="Enable verbose mode",
    )
    parser.add_argument(
        "-V",
        "--version",
        action="version",
        version=f"%(prog)s {version('pyofscraperstash')}",
    )
    parser.add_argument(
        "--sanity-check",
        action="store_true",
        help="Sanity check the configuration file",
    )
    args = parser.parse_args()
    if not args.config_file:
        parser.print_help()
        exit(1)
    return args


async def main() -> None:
    global runtime_settings
    args: argparse.Namespace = parse_arguments()
    if args.metadata_modification_date and not args.after:
        args.after = args.metadata_modification_date
    logging.basicConfig(level=logging.DEBUG if args.verbose else logging.INFO)
    file_logger.debug("%s", pformat(args))
    logger.info("Loading configuration file: %s", args.config_file)
    load_config(args.config_file)
    runtime_settings["before"] = args.before or runtime_settings.get("before", None)
    runtime_settings["after"] = args.after or runtime_settings.get("after", None)
    runtime_settings["verbose"] = args.verbose or runtime_settings.get("verbose", False)
    runtime_settings["sanity_check"] = args.sanity_check or runtime_settings.get(
        "sanity_check", False
    )
    runtime_settings["metadata_modification_date"] = (
        args.metadata_modification_date or None
    )
    file_logger.debug("%s", pformat(runtime_settings))
    if runtime_settings["sanity_check"]:
        logger.info("Sanity check enabled")
        logger.info(f"Configuration file: {args.config_file}")
        logger.info(pformat(runtime_settings))
    logger.info("Loading models...")
    metadata_db_sets = await get_metadata_db_files(
        models=runtime_settings.get("models", {}),
        mod_date=runtime_settings.get("metadata_modification_date", None),
    )
    stash = StashInterface(
        {
            "scheme": runtime_settings["stashapp"]["scheme"],
            "host": runtime_settings["stashapp"]["host"],
            "port": runtime_settings["stashapp"]["port"],
            "ApiKey": runtime_settings["stashapp"]["api_key"],
        }
    )
    try:
        if len(metadata_db_sets) == 0:
            logger.error("No metadata database files found")
            exit(1)
        metadata_db_files = [db_file for _, db_file in metadata_db_sets]
        metadata_performers = [performer for performer, _ in metadata_db_sets]
        logger.info(f"Found {len(metadata_db_files)} metadata database files")
        stash_performers = await get_stash_performers(metadata_performers, stash)
        file_logger.debug("%s", pformat(stash_performers))
        if runtime_settings["sanity_check"]:
            for performer, performer_data in stash_performers.items():
                logger.info(
                    f"Performer: {performer} - Alias: {performer_data['alias_list']}"
                )
            try:
                input(
                    "If these match up press Enter to continue, or Ctrl-C to quit and fix your StashDB..."
                )
            except KeyboardInterrupt:
                logger.warning("Script interrupted. Exiting...")
                loop = asyncio.get_running_loop()
                loop.stop()
                exit(1)
        signal.signal(signal.SIGINT, signal_handler)
        stash_studios = {}
        async for performer in aiter_list(metadata_performers):
            stash_studios[performer] = await get_stash_studio(performer, stash)
            stash_studios[performer] = await verify_studio_url(
                stash_studios[performer], performer, stash
            )
            if (
                stash_studios[performer] is not None
                and runtime_settings["sanity_check"]
            ):
                stash_studios[performer] = await check_studio_description(
                    stash_studios[performer], performer, stash
                )
            stash_performers[performer]["studio"] = stash_studios[performer]
        for username, db_file in metadata_db_sets:
            logger.info(f"Processing performer: {username}")
            file_logger.info(f"Starting scan of {username}'s OF site directory")
            await scan_performer_directory(username, stash)
            if args.images_only:
                await process_image_files(
                    db_file,
                    performer=stash_performers[username],
                    username=username,
                    stash=stash,
                )
            elif args.scenes_only:
                await process_scene_files(
                    db_file,
                    performer=stash_performers[username],
                    username=username,
                    stash=stash,
                )
            else:
                await process_image_files(
                    db_file,
                    performer=stash_performers[username],
                    username=username,
                    stash=stash,
                )
                await process_scene_files(
                    db_file,
                    performer=stash_performers[username],
                    username=username,
                    stash=stash,
                )
        gen_job = stash.metadata_generate(
            {
                "covers": True,
                "sprites": True,
                "previews": True,
                "imagePreviews": True,
                "previewOptions": {
                    "previewSegments": 12,
                    "previewSegmentDuration": 0.75,
                    "previewExcludeStart": "0",
                    "previewExcludeEnd": "0",
                    "previewPreset": "slow",
                },
                "markers": True,
                "markerImagePreviews": True,
                "markerScreenshots": True,
                "transcodes": False,
                "phashes": True,
                "interactiveHeatmapsSpeeds": True,
                "imageThumbnails": True,
                "clipPreviews": True,
                "overwrite": False,
            }
        )
        logger.info(f"Metadata generation job: {gen_job}")
        # running = True
        # while running:
        #     job_status = stash.find_job(gen_job)
        #     logger.info(f"Metadata generation status: {job_status}")
        #     file_logger.info(f"Metadata generation status: {job_status}")
        #     if job_status["status"] in ["FINISHED", "FAILED", "CANCELLED"]:
        #         running = False
        #         break
        #     else:
        #         await asyncio.sleep(5)
    except Exception as e:
        logger.exception(e, exc_info=True, stack_info=True)
        return
    finally:
        for conn in metadata_db_files:
            await conn.close()
    file_logger.info("Finished processing metadata databases")


if __name__ == "__main__":
    for handler in file_logger.handlers[:]:
        file_logger.removeHandler(handler)
        handler.close()

    # Create a new file handler that overwrites the log file
    file_handler = logging.FileHandler(LOG_FILE, mode="w", encoding="utf-8")
    file_handler.setFormatter(logging.Formatter(FORMAT))
    file_logger.addHandler(file_handler)
    file_logger.setLevel(logging.DEBUG)

    file_logger.info("Starting pyOFscraperStash")
    file_logger.info("Logging to %s", LOG_FILE)
    file_logger.info(f"Started at {datetime.now()}")
    file_logger.info("========================================\n\n\n")
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
    loop.run_until_complete(main())
    loop.close()
file_handler.close()
