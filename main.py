"""Import the metadata sqlite3 database generated by the OF-Scraper project into a Stash app instance."""

import asyncio
import json
import logging
import os
import sys
import time

# from datetime import date, datetime, timedelta, timezone
from functools import reduce
from pprint import PrettyPrinter, pformat
from string import Formatter
from typing import Any
from urllib.parse import urlparse

import yaml  # type: ignore[unused-ignore]
from stashapi.stashapp import StashInterface

# from urllib.parse import urlparse


logger = logging.getLogger(__name__)
FORMAT = (
    "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s-%(lineno)d - %(message)s"
)
LOG_FILE = "/home/ofscraper/.config/ofscraper/logging/post-debug.log"
file_logger = logging.getLogger("file_logger")
logging.addLevelName(5, "TRACE")
if os.path.exists(LOG_FILE):
    file_age = time.time() - os.path.getmtime(LOG_FILE)
    if file_age > 3600:  # 1 hour in seconds
        file_handler = logging.FileHandler(LOG_FILE, mode="w", encoding="utf-8")
    else:
        file_handler = logging.FileHandler(LOG_FILE, mode="a", encoding="utf-8")
else:
    file_handler = logging.FileHandler(LOG_FILE, mode="w", encoding="utf-8")
file_handler.setFormatter(logging.Formatter(FORMAT))
file_logger.addHandler(file_handler)
file_logger.setLevel(logging.DEBUG)
file_logger.propagate = False
ARGS_FORMAT = "%(asctime)s - %(name)s - %(funcName)s-%(lineno)d - %(message)s"
ARGS_FILE = "/home/ofscraper/.config/ofscraper/logging/post-args.log"
args_logger = logging.getLogger("args_logger")
if os.path.exists(ARGS_FILE):
    file_age = time.time() - os.path.getmtime(ARGS_FILE)
    if file_age > 3600:  # 1 hour in seconds
        args_handler = logging.FileHandler(ARGS_FILE, mode="w", encoding="utf-8")
    else:
        args_handler = logging.FileHandler(ARGS_FILE, mode="a", encoding="utf-8")
else:
    args_handler = logging.FileHandler(ARGS_FILE, mode="w", encoding="utf-8")
args_handler.setFormatter(logging.Formatter(ARGS_FORMAT))
args_logger.addHandler(args_handler)
args_logger.setLevel(logging.INFO)
args_logger.propagate = False
runtime_settings = {}
args = []
semaphore = asyncio.Semaphore(16)


class Default(dict[Any, Any]):
    def __missing__(self, key: str) -> str:
        return "{" + key + "}"


pprint = PrettyPrinter(indent=4, depth=2, compact=True).pprint


def load_config(config_file: str) -> None:
    global runtime_settings
    _, ext = os.path.splitext(config_file)
    with open(config_file) as file:
        if ext.lower() == ".json":
            data = json.load(file)
        elif ext.lower() == ".yaml" or ext.lower() == ".yml":
            data = yaml.safe_load(file)
        else:
            raise ValueError(
                f"Unsupported file extension {ext}. Please use .json or .yaml/.yml"
            )
    runtime_settings = data
    runtime_settings["config_file"] = config_file


# Stolen from https://stackoverflow.com/a/46890853 with some added type casting
def deep_get(
    dictionary: dict, keys: str, default: type[ValueError] | None = None
) -> Any:
    return reduce(
        lambda d, key: d.get(key, default) if isinstance(d, dict) else default,
        keys.split("."),
        dictionary,
    )


def format_directory(dir_type: str | None = None, **vars) -> str:
    global runtime_settings
    return_string: str = ""
    if dir_type is None:
        raise ValueError("dir_type is required")
    if dir_type not in [
        "stash_location",
        "dir_format_stash",
    ]:
        raise ValueError(f"Unsupported dir_type: {dir_type}")
    path_key = (
        f"of_scraper.folders.{dir_type}"
        if dir_type != "dir_format_stash"
        else "of_scraper.folders.dir_format"
    )
    try:
        return_string = deep_get(runtime_settings, path_key, ValueError)
    except ValueError as e:
        file_logger.warning(f"Error: {e}")
        exit(1)
    if "metadata_format" in dir_type or "dir_format" in dir_type:
        save_or_stash = (
            "save_location"
            if "metadata_format" in dir_type or "dir_format" == dir_type
            else "stash_location"
        )
        return_string = return_string.format_map(
            Default(dict(**vars, save_location=format_directory(save_or_stash)))
        )
    fieldnames = [fname for _, fname, _, _ in Formatter().parse(return_string) if fname]
    replacements = {fname: vars.get("missing_values", "**") for fname in fieldnames}
    return_string = return_string.format_map(Default(dict(**replacements)))
    return return_string


async def generate_metadata_for_scene(stash: StashInterface, scene_id: int) -> None:
    stash.metadata_generate(
        {
            "covers": True,
            "sprites": True,
            "previews": True,
            "imagePreviews": True,
            "previewOptions": {
                "previewSegments": 12,
                "previewSegmentDuration": 0.75,
                "previewExcludeStart": "0",
                "previewExcludeEnd": "0",
                "previewPreset": "slow",
            },
            "markers": True,
            "markerImagePreviews": True,
            "markerScreenshots": True,
            "transcodes": False,
            "phashes": True,
            "interactiveHeatmapsSpeeds": True,
            "imageThumbnails": True,
            "clipPreviews": True,
            "sceneIDs": [scene_id],
            "overwrite": False,
        }
    )


async def generate_metadata(stash: StashInterface) -> None:
    stash.metadata_generate(
        {
            "covers": True,
            "sprites": True,
            "previews": True,
            "imagePreviews": True,
            "previewOptions": {
                "previewSegments": 12,
                "previewSegmentDuration": 0.75,
                "previewExcludeStart": "0",
                "previewExcludeEnd": "0",
                "previewPreset": "slow",
            },
            "markers": True,
            "markerImagePreviews": True,
            "markerScreenshots": True,
            "transcodes": False,
            "phashes": True,
            "interactiveHeatmapsSpeeds": True,
            "imageThumbnails": True,
            "clipPreviews": True,
            "overwrite": False,
        }
    )


def scan_user(stash: StashInterface, username: str) -> None:
    global runtime_settings
    global args
    path = [format_directory("dir_format_stash", model_username=username)]
    file_logger.info(f"path: {pformat(path)}")
    job = stash.metadata_scan(paths=path)
    running_job = True
    prior_status: str = ""
    while running_job:
        job_status = stash.find_job(job)
        if job_status["status"] in ["FINISHED", "FAILED", "CANCELLED"]:
            running_job = False
            break
        else:
            if prior_status == job_status["status"] and job_status["status"] == "READY":
                prior_status = job_status["status"]
                time.sleep(0.5)
                continue
            try:
                file_logger.info(
                    f"Job status: {job_status['status']} - {job_status['description']} - Sub-Task: {job_status['subTasks']}"
                )
            except KeyError:
                file_logger.info(
                    f"Job status: {job_status['status']} - {job_status['description']}"
                )
            except TypeError:
                file_logger.info(
                    f"Job status: {job_status['status']} - {job_status['description']}"
                )
            prior_status = job_status["status"]
            time.sleep(0.5)


def check_postMedia_in_media(post_media: dict, media: list[dict]) -> bool:
    for m in media:
        if m.get("id", None) is None:
            continue
        if m["id"] == post_media["id"]:
            return True
    return False


def process_media_and_posts(
    stash: StashInterface, media: list[dict], posts: list[dict]
) -> None:
    global runtime_settings
    global args
    filtered_posts = []
    for post in posts:
        if post.get("mediaCount", 0) == 0:
            continue
        if post.get("media", None) is None:
            continue
        for post_media in post["media"]:
            if check_postMedia_in_media(post_media, media):
                if post not in filtered_posts:
                    filtered_posts.append(post)
                break
    for post in filtered_posts:
        args_logger.info(pformat(f"Post: {post}"))
        file_logger.info(pformat(f"Post: {post}"))
        if post.get("responseType", None) == "profile":
            file_logger.info("Profile")
            continue


def process_post_user() -> None:
    global runtime_settings
    global args
    with open(args[2]) as file:
        provided_data = json.load(file)
    args_logger.info(pformat(f"Provided Data: \n{pformat(provided_data)}"))
    username = provided_data["username"]
    user_id = provided_data["model_id"]
    file_logger.info(f"Username: {username}")
    file_logger.info(f"User ID: {user_id}")
    stash = StashInterface(
        {
            "scheme": runtime_settings["stashapp"]["scheme"],
            "host": runtime_settings["stashapp"]["host"],
            "port": runtime_settings["stashapp"]["port"],
            "ApiKey": runtime_settings["stashapp"]["api_key"],
        }
    )
    scan_user(stash, username)
    file_logger.info("Beginning of media looping")
    media = provided_data.get("media", [])
    posts = provided_data.get("posts", [])
    if len(media) > 0:
        file_logger.info(f"Media Count: {len(media)}")
        process_media_and_posts(stash, media, posts)
    contents = media + posts
    for content in contents:
        if (
            content.get("responseType", None) == "profile"
            or content.get("responsetype", None) == "profile"
        ):
            file_logger.info("Profile")
            continue
        # file_logger.info(pformat(f"Content: {content}"))
        if content.get("canPurchase", False):
            file_logger.info(
                "Message content still available for purchase - skipping\n\n"
            )
            continue
        if content.get("price", 0) > 0 and content.get("canViewMedia", False):
            file_logger.info("Content is paid and can be viewed")
        file_logger.info(f"Media Count: {content.get('mediaCount', 0)}")
        if content.get("mediaCount", 0) == 0:
            file_logger.info("No media available")
            file_logger.info(f"Media in dict: {len(content.get('media', []))}\n\n")
            continue
        file_logger.info(f"Media Length: {len(content.get('media', []))}")
        for media in content.get("media", []):
            if media.get("type", None) == "video":
                filename: str = ""
                file_logger.info("Video")
                if media.get("canView", False):
                    file_logger.info("Video can be viewed")
                else:
                    file_logger.info("Video cannot be viewed")
                    continue
                if media.get("source", None) is not None:
                    if media["source"].get("source", None) is not None:
                        source_url = urlparse(media["source"]["source"])
                        source_path = source_url.path
                        file_logger.info(pformat(f"Source: {source_path}"))
                        source_array = source_path.split("/")
                        source_filename = source_array[-1]
                        file_logger.info(pformat(f"Filename: {source_filename}"))
                        filename = source_filename
                    else:
                        file_logger.info("No source available")
                if media.get("files", None) is not None and filename == "":
                    file = media.get("files", {})
                    file_logger.info(pformat(f"File: {file}"))
                    if file.get("drm", False):
                        file_logger.info("File is DRM protected")
                        if file["drm"].get("manifest", None) is not None:
                            file_logger.info(
                                pformat(f"Manifest: {file['drm']['manifest']}")
                            )
                            hls = file["drm"]["manifest"].get("hls", None)
                            if hls is not None:
                                hls_url = urlparse(hls)
                                hls_path = hls_url.path
                                file_logger.info(pformat(f"HLS: {hls_path}"))
                                hls_array = hls_path.split("/")
                                hls_filename = hls_array[-1]
                                file_logger.info(pformat(f"Filename: {hls_filename}"))
                                filename = hls_filename
                            dash = file["drm"]["manifest"].get("dash", None)
                            if dash is not None:
                                dash_url = urlparse(dash)
                                dash_path = dash_url.path
                                file_logger.info(pformat(f"DASH: {dash_path}"))
                                dash_array = dash_path.split("/")
                                dash_filename = dash_array[-1]
                                file_logger.info(pformat(f"Filename: {dash_filename}"))
                filename = os.path.splitext(filename)[0]
                file_logger.info(pformat(f"Query File basename: {filename}"))
                if filename == "":
                    continue
                scenes = stash.find_scenes(filter={"per_page": -1}, q=filename)
                if scenes is None:
                    file_logger.info("No scenes found")
                    continue
                if len(scenes) == 0:
                    file_logger.info("No scenes found")
                    continue
                if len(scenes) > 5:
                    file_logger.info("Too many scenes found")
                    continue
                file_logger.info(pformat(f"Scenes: {len(scenes)}"))
                for scene in scenes:
                    asyncio.run(generate_metadata_for_scene(stash, scene["id"]))
            elif media.get("type", None) == "photo":
                file_logger.info("Photo")
                if media.get("canView", False):
                    file_logger.info("Image can be viewed")
                else:
                    continue
                if media.get("source", None) is not None:
                    if media["source"].get("source", None) is not None:
                        file_logger.info(
                            pformat(f"Source: {urlparse(media['source']['source'])}")
                        )
                        continue
                    else:
                        file_logger.info("No source image available")
                if media.get("files", None) is not None:
                    for file in media["files"]:
                        file_logger.info(pformat(f"File: {file}"))
            else:
                file_logger.info(f"Unknown media type - {media.get('type', None)}")
                continue
            # file_logger.info(pformat(f"Media: {media}"))
        file_logger.info("End of Content\n\n\n")
    file_logger.info(f"End of user {username}")
    file_logger.info("End of main\n\n\n\n\n")


def process_post_loop() -> None:
    global runtime_settings
    global args
    with open(args[2]) as file:
        provided_data = json.load(file)
    args_logger.info(pformat(f"Provided Data: \n{pformat(provided_data)}"))
    stash = StashInterface(
        {
            "scheme": runtime_settings["stashapp"]["scheme"],
            "host": runtime_settings["stashapp"]["host"],
            "port": runtime_settings["stashapp"]["port"],
            "ApiKey": runtime_settings["stashapp"]["api_key"],
        }
    )
    for user in provided_data["users"]:
        file_logger.info("Beginning of user scan")
        username: str = user.get("username", "Missing")
        user_id: int = user.get("id", 0)
        file_logger.info(f"Username: {username}")
        file_logger.info(f"User ID: {user_id}")
        scan_user(stash, username)
        file_logger.info(f"End of user {username}")
        file_logger.info(
            f"Expiration: {user.get('subscribedByData', {}).get('expiredAt', 'N/A')}"
        )
    file_logger.info("End of users scan")
    file_logger.info("Starting generation of metadata")


def main() -> None:
    global runtime_settings
    global args
    load_config("/home/ofscraper/.config/ofscraper/post-config.yml")
    args = sys.argv
    args_logger.info(pformat(f"Args Length: {len(sys.argv)}"))
    args_logger.info(pformat(f"Args: {args}"))
    if args[1] == "user":
        file_logger.info("User Mode")
        file_logger.info(pformat(f"Args: {args[2]}"))
        process_post_user()
    elif sys.argv[1] == "loop":
        file_logger.info("Loop Mode")
        file_logger.info(pformat(f"Args: {args[2]}"))
        process_post_loop()
    else:
        args_logger.info(pformat(f"Mode: {sys.argv[1]}"))
        raise ValueError("Invalid mode")


if __name__ == "__main__":
    main()
